\textbf{Resumen}:

Resumen ES.

\textbf{Palabras claves}: clave1, clave2, ..., clave5

\vspace{1cm}
\begin{center}
  \rule{0.5\textwidth}{.4pt}
\end{center}
\vspace{1cm}

\textbf{Abstract}:


Benchmarks are widely used for measuring and comparing performance of LLMs in different areas, including logical puzzles, factual accuracy, and coding tasks among others. 
The use of benchmarks allows developers to fine-tune a model, and a user to choose a model that better fits their needs or adjust its responses by changing a system prompt and parameters like temperature. 
This requires a lot of benchmark runs. Running a benchmark is costly, time- and energy-consuming. Also, it is often inefficient, as they may contain problems that are not relevant for the use case of a model being adjusted. 

So the goal of this work is to explore existing benchmarks, and design and develop an approach to benchmarking that is highly customizable and extendible, time-efficient, eco-friendly. As a result, a modular benchmark was developed, that allows to add and modify test cases using a user interface, and configure a benchmark run using task filters and toggles for enabling or disabling specific checks. Also, the output of the benchmark allows to examine the results in detail. 

\textbf{Keywords}: keyword1, keyword2, ..., keyword5
