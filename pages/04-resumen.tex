\textbf{Resumen}:

Resumen ES.

\textbf{Palabras claves}: clave1, clave2, ..., clave5

\vspace{1cm}
\begin{center}
  \rule{0.5\textwidth}{.4pt}
\end{center}
\vspace{1cm}

\textbf{Abstract}:

Large Language Models (LLMs) are increasingly used for code generation, but their evaluation remains challenging.
Existing benchmarks are often rigid, resource-intensive, and prone to issues such as task saturation and data leakage.
This thesis analyzes the limitations of current benchmarking practices and introduces a modular, customizable framework for evaluating LLMs in programming tasks.
The proposed system enables flexible task selection, fine-grained configuration, and integration of environmental and cost considerations.
An interactive interface supports task management and detailed result analysis, making benchmarking more practical and sustainable.
The main contributions include a review of existing benchmarks, the design and implementation of a modular framework, and recommendations for improving benchmarking practices.
The results suggest that customizable and eco-aware benchmarks can provide more relevant insights while reducing computational overhead.

% Benchmarks are widely used for measuring and comparing performance of LLMs in different areas, including logical puzzles, factual accuracy, and coding tasks among others.
% The use of benchmarks allows developers to fine-tune a model, and a user to choose a model that better fits their needs or adjust its responses by changing a system prompt and parameters like temperature.
% This requires a lot of benchmark runs. Running a benchmark is costly, time- and energy-consuming. Also, it is often inefficient, as they may contain problems that are not relevant for the use case of a model being adjusted.
% So the goal of this work is to explore existing benchmarks, and design and develop an approach to benchmarking that is highly customizable and extendible, time-efficient, eco-friendly. As a result, a modular benchmark was developed, that allows to add and modify test cases using a user interface, and configure a benchmark run using task filters and toggles for enabling or disabling specific checks. Also, the output of the benchmark allows to examine the results in detail.

\textbf{Keywords}: keyword1, keyword2, ..., keyword5
