% Checklist and TO DOs
% Describe metodology of the work with sources
% why do we need benchmarks
    % Frontier language models are not reliable on simple tasks \cite{vendrow2025largelanguagemodelbenchmarks}
% Classification of Benchmarks
% add Pros and Cons of my approach compared with anothers
% why do most benchmark use fixed sets of tasks
% why do we need a modular benchmark
% What do benchmarks check
  % Categories of tasks: Math, Logic, Table understanding,  Reading comprehension, Commonsense reasoning, Vision \cite{vendrow2025largelanguagemodelbenchmarks}
%
% Why did I chose Java and Spring Boot instead of Python (because Spring AI is great), and React
% Mention that I used OpenRouter, but any provided supported by Spring AI could be used
%
% Focus more on the concept of modular and customizable benchmarks, than in the implementation
% Describe bigcode-evaluation-harness and lm-evaluation-harness, and what lacks in them

\chapter{Introduction}

% What is missing or should be improved:
% Methodology Overview: There is no brief mention of the approach or methods used in the work. Add a short paragraph summarizing the methodology (e.g., system design, implementation, evaluation).
% Нужно описать, что я нашёл и использовал уже существующие Critical Review по теме в качетсве базы для анализа существующей литературы, затем следовал ссылкам на статьи, которые мне нужны. Последний подходящий Critical Review был опубликован в середине 2024 года, так что затем я сделал поиск по статьям за 2025 года и вторую половину 2024 года по ключевым словам, связанным с Benchmarking и LLM, выбрав среди них актуальные, отвечая на вопросы исследования. Затем, на основании собранной информации, я добавил свои собственные идеи и предложения по улучшению бенчмарков.

% ВАЖНО !!! Нужно добавить список вопросов, на которые я отвечал в ходе исследования литературы.!!

% Contributions: While objectives are listed, a concise summary of the main contributions (what is new or improved) should be added at the end of the introduction.

Large Language Models (LLMs) have revolutionized software development through AI-assisted programming tools like GitHub Copilot. However, evaluating and fine-tuning these models requires extensive benchmarking, which comes at significant computational, financial, and environmental costs. Current benchmarks often contain irrelevant tasks and provide limited customization options, making them inefficient for specific use cases.

This work explores existing benchmarks for code-generating LLMs and proposes a novel approach: modular and customizable benchmarks that can be tailored to specific needs while remaining cost-effective and environmentally conscious.

% Methodology Overview
This thesis began with a comprehensive literature review, using recent critical reviews as a foundation for analyzing the state of LLM benchmarking. The review was extended by following citations to relevant articles published in late 2024 and 2025, focusing on keywords related to benchmarking and LLMs. The findings from this literature analysis informed the design and implementation of a modular benchmarking system, which was then evaluated for efficiency, flexibility, and environmental impact.

% Research Questions
The literature review and subsequent research were guided by the following questions:
\begin{itemize}
    \item What are the main limitations of current LLM benchmarks for code generation?
    % \item How does benchmark saturation and task leaking affect the reliability of evaluation?
    \item What metrics best reflect real-world usability and code quality?
    \item How can benchmarks be made customizable for different user needs?
    \item What is the environmental impact of repeated benchmarking, and how can it be reduced?
\end{itemize}

% Contributions Summary
\textbf{Main contributions of this work:}
\begin{itemize}
    \item A critical analysis of existing LLM code generation benchmarks and their shortcomings.
    \item The design and implementation of a modular, customizable benchmarking framework.
    \item Integration of environmental and cost considerations into the benchmarking process.
    \item An interactive web interface for configuring benchmarks and analyzing results.
    \item Recommendations for future benchmarking practices based on empirical findings.
\end{itemize}

\section{Problem Statement}

Running benchmarks is essential for LLM developers, researchers and users. Developers and researchers need benchmarks to fine-tune their models or compare approaches, while users rely on them to select appropriate models and adjust parameters like temperature and system prompts. However, this process faces several challenges:
% Fine-tune key parameters like temperature, top_p, top_k to control model creativity vs determinism.
Cost and Resource Consumption: Each benchmark run consumes significant computational resources, time, and energy. As noted in recent studies, the environmental impact of repeated benchmark runs is becoming a growing concern.
% TO-DO: add a reference to the study on environmental impact

Lack of Customization: Most benchmarks are developed as fixed sets of tasks, making them difficult to adapt for specific use cases. This limitation was highlighted in the SWE-bench study~\cite{jimenez2024swebenchlanguagemodelsresolve}, which emphasized the need for more flexible evaluation methods.
% Развить мысль, как SWE сам решает эту проблему и каким образом

Inefficient Task Selection: Many benchmarks include tasks that may be irrelevant for specific applications, leading to wasted resources. Recent research \cite{vendrow2025largelanguagemodelbenchmarks} has shown that benchmark saturation often results in running unnecessary tests that all modern LLMs pass easily.

\section{Objectives}

The main objectives of this work are:
\begin{enumerate}
    \item Analyze existing benchmarks and their limitations
    \item Design a modular benchmark system that allows:
    \begin{itemize}
        \item Custom task selection and filtering
        \item Configuration of testing criteria
        \item Support for multiple programming languages and task types
        \item Integration with CI/CD pipelines
    \end{itemize}
    \item Implement an interactive web interface for benchmark configuration and result analysis
    \item Develop a cost-efficient and environmentally conscious approach to benchmarking
\end{enumerate}

\chapter{State of the Art}

\section{Evolution of Code Generation Benchmarks}

The landscape of LLM benchmarks has evolved significantly since the introduction of HumanEval \cite{chen2021evaluatinglargelanguagemodels}. This pioneering benchmark set a standard for evaluating code generation capabilities, but its limitations became apparent as LLMs advanced.

Recent developments like HumanEval Pro~\cite{yu2024humanevalprombpppro} have introduced more sophisticated testing approaches. For instance, their multi-step evaluation process tests an LLM's ability to work with its own generated code, revealing limitations in some models that perform well on simpler tasks.
% Хочу сказать иначе: задачки в HumanEval уже общеизвестны, так что ЛЛМ натренированы на их решениях. Зато HumanEval Pro добавили дополнительный шаг в задачу, что продемонстрировало, что даже самые современные LLMs могут не справляться с незнакомыми задачами, или переиспользовать написанный ими же код, вызывая его в качестве функции. Таким образом, разработка новых задач важна, и нужно предоставить пользователю бенчмарка удобный способ добавлять и модифицировать задачи в бенчмарке.


\section{Types of LLM Benchmarks}

% add classifications

While academic benchmarks like HumanEval focus on controlled, isolated tasks, initiatives like SWE-bench \cite{jimenez2024swebenchlanguagemodelsresolve} have moved toward real-world scenarios. This shift reflects a growing recognition that LLM evaluation should encompass the complexity of actual software development.

The InterCode framework \cite{yang2023intercodestandardizingbenchmarkinginteractive} introduced interactive environments using Docker, enabling evaluation of LLMs in realistic development scenarios with compilation and runtime feedback. This approach more closely mirrors actual developer workflows but comes with increased computational overhead.

In~\cite{chi2025copilotarenaplatformcode} the authors developed a plugin for IDE that offers the user two code completion options from two different LLMs and records which option the user chose. This allowed them to compare the performance of different LLMs in real-world coding tasks, providing valuable insights into user preferences and model effectiveness.
Another article~\cite{mozannar2024readinglinesmodelinguser} finds out that a user can often accept a first proposed solution in order to see it with proper syntax highlighting and be able to understand it better. But later he could even remove the suggestion and waits for a repeated completion or simply writes the piece of the code themself. Although that should not be considered a significant flaw of the benchmark, it could partially skew the results.

Most benchmarks are very limited in the output they provide. For example, SWE leaderboard~\cite{swebenchSWEbenchLeaderboards} is created based on a single number that does not reflect the types of tasks that the model is better or worse at solving~\cite{miah2024usercentricevaluationcode}. Thus, a user might choose a suboptimal model for their specific needs, resulting in lower performance or higher cost. This is partially countered by websites that aggregate results on several benchmarks~\cite{vellumLeaderboard2025} which can give a very high-level picture.

\section{Problems in LLM Benchmarking}

Benchmark saturation mentioned above is partially explained by advances in models, but it can also be attributed to information \textbf{leaking}: the popular and publicly available benchmarks appear in the training datasets accompanied with the golden solutions. 
Even then, it doesn't mean that an LLM won't struggle when presented with the same task. 
When changing the task phrasing while keeping the semantic consistent, there is a 4.5 percet drop in solvability, showing that the models remember the phrasing of the descriptions in the original dataset. \cite{uniyal2024one}

One of the important aspects of LLM evaluation is the choice of the metrics.
For code quality, there are BLEU, CodeBLEU, RUBY, ROUGE-L, METEOR, ChrF\@.
They assess the similarity of the generated code to the golden solution, taking into account the properties of source code.
Evtikhiev et al.~\cite{evtikhiev2023out} takes 6 metrics, commonly used in papers.
However, the authours conduct a study, comparing the results of metrics with human evaluation of the solutions.
The results suggest that none of the analyzed metrics can correctly emulate human judgment, but ChrF metric is considered better than the others commonly used in papers.

A paper by \cite{crupi2025effectiveness} looks into an approach of using LLM to evaluate the quality of the solution generated by another model (LLM-as-a-judge approach).
As a result, they come to a conclusion that LLM-as-a-judge is a substantial improvement over mentioned metrics, and GPT-4-turbo can mimic closely a human evaluation.

% remove or rephrase: Recent studies have revealed the complexity of user interaction with AI programming assistants. Research by \cite{chen2025needhelpdesigningproactive} identified specific moments when developers expect proactive suggestions from LLMs, while \cite{mozannar2024readinglinesmodelinguser} classified 12 distinct states of interaction with tools like GitHub Copilot.
% И что с того, что идентифицировал? Тогда нужно развить мысль, но вообще я не решаю эту проблему в своей работе


\section{Environmental impact of Benchmarking}

Repeated training and benchmarking of LLMs require substantial computational resources, leading to significant electricity consumption and carbon emissions. This environmental impact is increasingly important in the context of global efforts to reduce carbon footprints. We will want for benchmarks to account for these factors and encourage more sustainable evaluation practices.

% Тут надо сказать, что при разработке и fine-tuning LLM, используются значительные мощности, в том числе при повторяющемся запуске бенчмарков. Это приводит к значительным затратам на электроэнергию и углеродные выбросы, что становится все более важным вопросом в свете глобальных усилий по снижению воздействия на окружающую среду. Добавить какое-то исследование на эту тему.

There are leaderboards that account for $CO_2$ emissions, such as Hugging Face \cite{huggingfaceCalculation}, which tracks the carbon footprint of models. However, these metrics are often not integrated into traditional benchmarks, leading to a lack of awareness about the environmental impact of LLM evaluation practices.

The most common metric in benchmarks is pass@k that measures the percentage of correct solutions among the $k$ solutions generated by the model.
This implies that for each task in the benchmark dataset, a model repeatedly generates a number of solutions, just to receive a single numeric result to use for a metric.
This metric is used in ClassEval, MBPP, MathQA-Python, CoderEval, and HumanEval+.
Notably, HumanEval and HumanEval+ use $k=100$ (\texttt{pass@100}).
However, as Miah and Zhu~\cite{miah2024usercentricevaluationcode} pointed out, users do not normally run the LLM several times, so pass@k does not reflect its usability.

\section{Existing Benchmarking Frameworks and Their Limitations}

% TO DO more detail and a comparison table

Apart from the benchmarks themselves, there are several frameworks that facilitate LLM evaluation.
These frameworks provide tools for running benchmarks and collecting results.

Two widely used benchmarking frameworks are \textbf{bigcode-evaluation-harness}~\cite{bigcode-evaluation-harness} and \textbf{lm-evaluation-harness}~\cite{githubGitHubEleutherAIlmevaluationharness}.
Both provide tools for running standardized benchmarks on LLMs, but they have notable limitations.
The lm-evaluation-harness is more general-purpose and supports a broader range of language tasks, yet it also relies on fixed task sets and lacks modularity for user-defined benchmarks.
Neither framework provides built-in support for environmental metrics or fine-grained task selection, highlighting the need for more flexible and sustainable benchmarking solutions.

\begin{table}[h]
\centering
\begin{tabular}{|p{4cm}|p{5cm}|p{5cm}|}
  \hline
        \textbf{Feature} & \textbf{bigcode-evaluation-harness} & \textbf{lm-evaluation-harness} \\
        \hline
        Evaluation of multiple LLMs & One model per run & One model per run \\
        \hline
        Available configuration & Task dataset name, Number of tasks, Temperature, Saving LLM responses, Limit of LLM response & TO-DO \\
        \hline
        Included benchmarks & Common benchmark tasks & Limited, fixed task sets \\
        \hline
        Defining new tasks & Requires source code modification & Requires source code modification \\
        \hline
        Run interface & CLI-based, no GUI & CLI-based, no GUI \\
        \hline
        Result analysis & Overall numeric metric and LLM responses saved as a file &  \\
        \hline
        Visualization & No visualization tools & No visualization tools \\
        \hline
    \end{tabular}
    \caption{Comparison of bigcode-evaluation-harness and lm-evaluation-harness}
    \label{tab:framework-comparison}
\end{table}

\chapter{Problem Description}

\section{Analysis of Current Limitations}

The limitations of existing benchmarks extend beyond just inefficiency. Studies like  have identified several critical issues:

% TO DO: check if all that is mentioned in the paper
\begin{itemize}
    \item Benchmark saturation: Many tasks become irrelevant as models improve \cite{vendrow2025largelanguagemodelbenchmarks}
    \item Task leaking: LLMs are trained on benchmark tasks solutions, making them obsolete \cite{vendrow2025largelanguagemodelbenchmarks}
    \item Limited feedback: Most benchmarks provide only pass/fail results
    \item High costs: Running comprehensive benchmarks is expensive and time-consuming
    \item Environmental impact: Repeated runs contribute to unnecessary carbon emissions
    \item Inflexibility: Fixed task sets don't adapt to specific needs
    \item Error-prone: benchmarks contain up to 5 percent of mislabelled or erroneous tasks \cite{vendrow2025largelanguagemodelbenchmarks}
\end{itemize}


% Describe how do I address these limitations in my work

\section{Requirements for a Solution}

Based on the analyzed limitations and user needs, we identified key requirements for a more effective benchmarking approach:

Modularity:
\begin{itemize}
    \item Support for multiple programming languages
    \item Ability to add/modify tasks easily
    \item Configurable testing criteria
\end{itemize}

Efficiency:
\begin{itemize}
    \item Task filtering capabilities
    \item Optimized resource usage
    \item Quick feedback loops
\end{itemize}

User Experience:
\begin{itemize}
    \item Interactive configuration interface
    \item Detailed result analysis
    \item Integration with development workflows
\end{itemize}

The benchmark should:
\begin{itemize}
    \item Provide a way to analyse individual task failures
    \item Allow users to select tasks based on their specific needs
    \item Get the most information from each generated solution
    \item ...
\end{itemize}

\chapter{Proposed Solution}

\section{Architecture Overview}

Our solution combines three main components:
\begin{itemize}
    \item A Spring Boot backend with MVC architecture
    \item A React-based frontend for configuration and visualization
    \item Docker environments for isolated task execution
\end{itemize}

\begin{figure}[h]
    \centering
    % Add architecture diagram placeholder
    \caption{System Architecture Overview}
    \label{fig:architecture}
\end{figure}

\section{Task Dataset Format}

Tasks are defined in YAML format, allowing for easy modification and extension:

% Example structure (full examples in Appendix)
% TODO put into Appendix
\begin{verbatim}
tasks:
  - name: "Example Task"
    type: "implementation"
    difficulty: "medium"
    languages: ["python", "java"]
    parameters:
      use_libraries: false
      generate_tests: true
\end{verbatim}

\section{Interactive Configuration}

The React frontend provides:
- Task filtering by type, difficulty, and language
- Testing criteria selection
- Resource usage configuration
- Result visualization

\chapter{Implementation Details}

\section{Project Structure}

The project follows a modern microservices architecture with separate backend and frontend applications:

\begin{verbatim}
Project Root
|-- exec_configs/    # Execution configuration YAML files
|-- task_sources/    # Task source YAML files
|-- bench_status/    # Benchmark status files
|-- bench_results/   # Benchmark results
|-- frontend/        # React application
`-- src/             # Spring Boot application
\end{verbatim}

\section{Backend Development}

The Spring Boot application serves as the core of the system, providing:

\subsection{REST API Endpoints}
The backend exposes a comprehensive RESTful API:
\begin{itemize}
    \item \texttt{/api/configs}: Configuration file management
    \item \texttt{/api/tasks}: Task source file operations
    \item \texttt{/api/benchmarks}: Benchmark execution control
    \item \texttt{/api/status}: Status monitoring
    \item \texttt{/api/results}: Result retrieval and analysis
\end{itemize}

\subsection{Core Functionality}
\begin{itemize}
    \item Task processing and execution in isolated environments
    \item Docker container management for language-specific runtimes
    \item File-based storage system for configurations and results
    \item Integration with LLM judges through Spring AI
\end{itemize}

\begin{figure}[h]
    \centering
    % Add backend architecture diagram
    \caption{Backend Component Architecture}
    \label{fig:backend-arch}
\end{figure}

\section{Frontend Development}

The React frontend, built with TypeScript, provides an intuitive interface for:

\subsection{User Interface Components}
\begin{itemize}
    \item Configuration file upload and management
    \item Task source file handling
    \item Real-time benchmark monitoring
    \item Result visualization and analysis
\end{itemize}

\subsection{Interactive Features}
The application enables users to:
\begin{itemize}
    \item Upload and manage YAML configurations
    \item Configure benchmark parameters interactively
    \item Monitor benchmark progress in real-time
    \item Analyze and export results
\end{itemize}

\begin{figure}[h]
    \centering
    % Add frontend screenshot
    \caption{Frontend Interface Overview}
    \label{fig:frontend-ui}
\end{figure}

\section{Deployment}

Both components support flexible deployment options:

\subsection{Local Development}
\begin{verbatim}
# Backend
mvn spring-boot:run

# Frontend
cd frontend
npm install
npm run dev
\end{verbatim}

\subsection{Docker Deployment}
The system includes Docker configurations for containerized deployment:

\begin{verbatim}
# Backend container
docker build -t ai-benchmark-backend -f Dockerfile.backend .
docker run -p 8080:8080 ai-benchmark-backend

# Frontend container
docker build -t ai-benchmark-frontend -f Dockerfile.frontend .
docker run -p 5173:5173 ai-benchmark-frontend
\end{verbatim}

\chapter{Results and Evaluation}

\section{Performance Analysis}

We evaluated our solution against traditional benchmarks:

\begin{table}[h]
    \centering
    % Add comparison table placeholder
    \caption{Comparison with Traditional Benchmarks}
    \label{tab:comparison}
\end{table}

Key metrics:
\begin{itemize}
    \item Execution time per task
    \item Resource usage (CPU, memory)
    \item Cost per benchmark run
    \item Environmental impact (estimated $CO_2$ emissions)
\end{itemize}

\section{Case Studies}

We present three scenarios demonstrating the system's flexibility:
\begin{enumerate}
    \item Fine-tuning an LLM for specific programming tasks
    \item Evaluating code style consistency
    \item Testing real-world project integration
\end{enumerate}

\chapter{Conclusions}

\section{Contributions}

Our modular benchmark approach offers several advantages:
\begin{itemize}
    \item Customizable evaluation scenarios
    \item Reduced resource consumption
    \item Improved feedback quality
    \item Easy integration with development workflows
\end{itemize}

\section{Future Work}

Potential improvements include:
\begin{itemize}
    \item Additional programming language support
    \item Enhanced visualization tools
    \item CI/CD pipeline integration
    \item Advanced metrics collection
\end{itemize}

\section{Impact}

This work contributes to more sustainable and efficient LLM evaluation practices, potentially reducing both costs and environmental impact while providing more meaningful results.
