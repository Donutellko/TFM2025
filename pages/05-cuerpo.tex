% Checklist and TO DOs
% Describe metodology of the work with sources
% why do we need benchmarks
% Classification of Benchmarks
% add Pros and Cons of my approach compared with anothers
% why do most benchmark use fixed sets of tasks
% why do we need a modular benchmark
%
% Why did I chose Java and Spring Boot instead of Python (because Spring AI is great), and React
% Mention that I used OpenRouter, but any provided supported by Spring AI could be used
%
% Focus more on the concept of modular and customizable benchmarks, than in the implementation
% Describe bigcode-evaluation-harness and lm-evaluation-harness, and what lacks in them

\chapter{Introduction}

% What is missing or should be improved:
% Methodology Overview: There is no brief mention of the approach or methods used in the work. Add a short paragraph summarizing the methodology (e.g., system design, implementation, evaluation).
% Contributions: While objectives are listed, a concise summary of the main contributions (what is new or improved) should be added at the end of the introduction.

Large Language Models (LLMs) have revolutionized software development through AI-assisted programming tools like GitHub Copilot. However, evaluating and fine-tuning these models requires extensive benchmarking, which comes at significant computational, financial, and environmental costs. Current benchmarks often contain irrelevant tasks and provide limited customization options, making them inefficient for specific use cases.

This work explores existing benchmarks for code-generating LLMs and proposes a novel approach: modular and customizable benchmarks that can be tailored to specific needs while remaining cost-effective and environmentally conscious.

\section{Problem Statement}

Running benchmarks is essential for both LLM developers and users. Developers need benchmarks to fine-tune their models, while users rely on them to select appropriate models and adjust parameters like temperature and system prompts. However, this process faces several challenges:
% Fine-tune key parameters like temperature, top_p, top_k to control model creativity vs determinism.
Cost and Resource Consumption: Each benchmark run consumes significant computational resources, time, and energy. As noted in recent studies \cite{ ???? Cantu-Paz98asurvey ?????}, the environmental impact of repeated benchmark runs is becoming a growing concern.

Lack of Customization: Most benchmarks are developed as fixed sets of tasks, making them difficult to adapt for specific use cases. This limitation was highlighted in the SWE-bench study \cite{jimenez2024swebenchlanguagemodelsresolve}, which emphasized the need for more flexible evaluation methods.

Inefficient Task Selection: Many benchmarks include tasks that may be irrelevant for specific applications, leading to wasted resources. Recent research \cite{vendrow2025largelanguagemodelbenchmarks} has shown that benchmark saturation often results in running unnecessary tests that all modern LLMs pass easily.

\section{Objectives}

The main objectives of this work are:
\begin{enumerate}
    \item Analyze existing benchmarks and their limitations
    \item Design a modular benchmark system that allows:
    \begin{itemize}
        \item Custom task selection and filtering
        \item Configuration of testing criteria
        \item Support for multiple programming languages and task types
        \item Integration with CI/CD pipelines
    \end{itemize}
    \item Implement an interactive web interface for benchmark configuration and result analysis
    \item Develop a cost-efficient and environmentally conscious approach to benchmarking
\end{enumerate}

\chapter{State of the Art}

\section{Evolution of Code Generation Benchmarks}

The landscape of LLM benchmarks has evolved significantly since the introduction of HumanEval \cite{chen2021evaluatinglargelanguagemodels}. This pioneering benchmark set a standard for evaluating code generation capabilities, but its limitations became apparent as LLMs advanced.

Recent developments like HumanEval Pro \cite{yu2024humanevalprombpppro} have introduced more sophisticated testing approaches. For instance, their multi-step evaluation process tests an LLM's ability to work with its own generated code, revealing limitations in some models that perform well on simpler tasks.

\section{Real-world vs. Academic Benchmarks}

While academic benchmarks like HumanEval focus on controlled, isolated tasks, initiatives like SWE-bench \cite{jimenez2024swebenchlanguagemodelsresolve} have moved toward real-world scenarios. This shift reflects a growing recognition that LLM evaluation should encompass the complexity of actual software development.

The InterCode framework \cite{yang2023intercodestandardizingbenchmarkinginteractive} introduced interactive environments using Docker, enabling evaluation of LLMs in realistic development scenarios with compilation and runtime feedback. This approach more closely mirrors actual developer workflows but comes with increased computational overhead.

\section{User Experience in AI-Assisted Programming}

Recent studies have revealed the complexity of user interaction with AI programming assistants. Research by \cite{chen2025needhelpdesigningproactive} identified specific moments when developers expect proactive suggestions from LLMs, while \cite{mozannar2024readinglinesmodelinguser} classified 12 distinct states of interaction with tools like GitHub Copilot.

These findings highlight a critical gap in current benchmarks: they often focus solely on code correctness while ignoring user experience factors that significantly impact real-world utility.

\chapter{Problem Description}

\section{Analysis of Current Limitations}

The limitations of existing benchmarks extend beyond just inefficiency. Studies like \cite{vendrow2025largelanguagemodelbenchmarks} have identified several critical issues:

\begin{itemize}
    \item Benchmark saturation: Many tasks become irrelevant as models improve
    \item Limited feedback: Most benchmarks provide only pass/fail results
    \item High costs: Running comprehensive benchmarks is expensive and time-consuming
    \item Environmental impact: Repeated runs contribute to unnecessary carbon emissions
    \item Inflexibility: Fixed task sets don't adapt to specific needs
\end{itemize}

\section{Requirements for a Solution}

Based on the analyzed limitations and user needs, we identified key requirements for a more effective benchmarking approach:

Modularity:
- Support for multiple programming languages
- Ability to add/modify tasks easily
- Configurable testing criteria

Efficiency:
- Task filtering capabilities
- Optimized resource usage
- Quick feedback loops

User Experience:
- Interactive configuration interface
- Detailed result analysis
- Integration with development workflows

\chapter{Proposed Solution}

\section{Architecture Overview}

Our solution combines three main components:
- A Spring Boot backend with MVC architecture
- A React-based frontend for configuration and visualization
- Docker environments for isolated task execution

\begin{figure}[h]
    \centering
    % Add architecture diagram placeholder
    \caption{System Architecture Overview}
    \label{fig:architecture}
\end{figure}

\section{Task Dataset Format}

Tasks are defined in YAML format, allowing for easy modification and extension:

% Example structure (full examples in Appendix A)
\begin{verbatim}
tasks:
  - name: "Example Task"
    type: "implementation"
    difficulty: "medium"
    languages: ["python", "java"]
    parameters:
      use_libraries: false
      generate_tests: true
\end{verbatim}

\section{Interactive Configuration}

The React frontend provides:
- Task filtering by type, difficulty, and language
- Testing criteria selection
- Resource usage configuration
- Result visualization

\chapter{Implementation Details}

\section{Project Structure}

The project follows a modern microservices architecture with separate backend and frontend applications:

\begin{verbatim}
Project Root
|-- exec_configs/    # Execution configuration YAML files
|-- task_sources/    # Task source YAML files
|-- bench_status/    # Benchmark status files
|-- bench_results/   # Benchmark results
|-- frontend/        # React application
`-- src/             # Spring Boot application
\end{verbatim}

\section{Backend Development}

The Spring Boot application serves as the core of the system, providing:

\subsection{REST API Endpoints}
The backend exposes a comprehensive RESTful API:
\begin{itemize}
    \item \texttt{/api/configs}: Configuration file management
    \item \texttt{/api/tasks}: Task source file operations
    \item \texttt{/api/benchmarks}: Benchmark execution control
    \item \texttt{/api/status}: Status monitoring
    \item \texttt{/api/results}: Result retrieval and analysis
\end{itemize}

\subsection{Core Functionality}
\begin{itemize}
    \item Task processing and execution in isolated environments
    \item Docker container management for language-specific runtimes
    \item File-based storage system for configurations and results
    \item Integration with LLM judges through Spring AI
\end{itemize}

\begin{figure}[h]
    \centering
    % Add backend architecture diagram
    \caption{Backend Component Architecture}
    \label{fig:backend-arch}
\end{figure}

\section{Frontend Development}

The React frontend, built with TypeScript, provides an intuitive interface for:

\subsection{User Interface Components}
\begin{itemize}
    \item Configuration file upload and management
    \item Task source file handling
    \item Real-time benchmark monitoring
    \item Result visualization and analysis
\end{itemize}

\subsection{Interactive Features}
The application enables users to:
\begin{itemize}
    \item Upload and manage YAML configurations
    \item Configure benchmark parameters interactively
    \item Monitor benchmark progress in real-time
    \item Analyze and export results
\end{itemize}

\begin{figure}[h]
    \centering
    % Add frontend screenshot
    \caption{Frontend Interface Overview}
    \label{fig:frontend-ui}
\end{figure}

\section{Deployment}

Both components support flexible deployment options:

\subsection{Local Development}
\begin{verbatim}
# Backend
mvn spring-boot:run

# Frontend
cd frontend
npm install
npm run dev
\end{verbatim}

\subsection{Docker Deployment}
The system includes Docker configurations for containerized deployment:

\begin{verbatim}
# Backend container
docker build -t ai-benchmark-backend -f Dockerfile.backend .
docker run -p 8080:8080 ai-benchmark-backend

# Frontend container
docker build -t ai-benchmark-frontend -f Dockerfile.frontend .
docker run -p 5173:5173 ai-benchmark-frontend
\end{verbatim}

\chapter{Results and Evaluation}

\section{Performance Analysis}

We evaluated our solution against traditional benchmarks:

\begin{table}[h]
    \centering
    % Add comparison table placeholder
    \caption{Comparison with Traditional Benchmarks}
    \label{tab:comparison}
\end{table}

Key metrics:
- Resource usage reduction
- Time efficiency
- Cost savings
- Environmental impact

\section{Case Studies}

We present three scenarios demonstrating the system's flexibility:
1. Fine-tuning an LLM for specific programming tasks
2. Evaluating code style consistency
3. Testing real-world project integration

\chapter{Conclusions}

\section{Contributions}

Our modular benchmark approach offers several advantages:
- Customizable evaluation scenarios
- Reduced resource consumption
- Improved feedback quality
- Easy integration with development workflows

\section{Future Work}

Potential improvements include:
- Additional programming language support
- Enhanced visualization tools
- CI/CD pipeline integration
- Advanced metrics collection

\section{Impact}

This work contributes to more sustainable and efficient LLM evaluation practices, potentially reducing both costs and environmental impact while providing more meaningful results.
