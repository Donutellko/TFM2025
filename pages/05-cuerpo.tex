\chapter{Introducción}

\section{Motivación}

\noindent Lorem ipsum dolor sit amet consectetur adipiscing elit integer, sapien condimentum nostra a metus tempus eget mi, rhoncus erat faucibus ad vivamus dictum interdum. Euismod dis dictum lacinia ullamcorper dictumst pharetra elementum potenti facilisi suscipit curae semper, nulla nostra mi sollicitudin lacus at sociosqu luctus eleifend pretium mus.

\begin{algorithm}[H]
  \caption{Ejemplo}\label{euclid}
  \begin{algorithmic}[1]
    \Procedure{Euclid}{$a,b$} \Comment{The g.c.d. of a and b}
        \State $r\gets a \bmod b$
        \While{$r\not=0$} \Comment{We have the answer if r is 0}
            \State $a \gets b$
            \State $b \gets r$
            \State $r \gets a \bmod b$
        \EndWhile\label{euclidendwhile}
        \State \textbf{return} $b$\Comment{The gcd is b}
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

Porta \cite{Cantu-Paz98asurvey} pulvinar fermentum faucibus lectus sodales etiam rutrum posuere dictum, ultrices torquent ad platea libero est elementum egestas tortor, a erat hendrerit sed ut aliquam vivamus habitant.

\tikzstyle{box} = [draw, rectangle, minimum height=3em, minimum width=3em]
\tikzstyle{cir} = [draw, circle, line width=0.5pt]
\tikzstyle{container} = [draw, rectangle, dashed, inner sep=1em]
\begin{figure*}[h]
  \centering
  \begin{tikzpicture}[auto, node distance=5cm]
      % Place nodes
      \node [box, fill={rgb:red,1;green,2;blue,5}] (node1) {Foo};
      \node [container, fit=(node1), label=north:{main}] (node1container) {};
      \node [cir, right=of node1] (node2) {Bar};
      % Connect nodes
      \draw [->, rounded corners=0.2cm] (node1) -- node {link} (node2);
  \end{tikzpicture}
  \caption{Diagrama de ejemplo.}
\end{figure*}

\newpage

\section{Estado del arte}

\cleardoublepage

\chapter{Diseño}

\section{Implementación}

\cleardoublepage

\chapter{Resultados}

\section{Conclusiones}

\section{Futuras líneas de trabajo}

\chapter{Overview}
% Resumen general del proyecto
% General overview of the project

\section{Technologies Used}
% Tecnologías utilizadas
% Technologies used
% [ ] Justify the choice of Spring Boot for backend development.
% [ ] Explain the use of MVC for structuring the application.
% [ ] Highlight the role of Spring AI in integrating LLMs.
% [ ] Discuss the use of React for building a dynamic frontend.
% [ ] Mention Docker for creating isolated environments for benchmarks.

\section{UI and Use Cases}
% Interfaz de usuario y casos de uso
% User interface and use cases
% [ ] Describe the main features of the React UI.
% [ ] Explain how users can configure benchmarks interactively.
% [ ] Provide examples of use cases, such as filtering tasks or exploring results.

\section{Backend-Frontend Interaction}
% Interacción entre el backend y el frontend
% Interaction between backend and frontend
% [ ] Describe the API endpoints for communication.
% [ ] Explain how the backend processes benchmark configurations.
% [ ] Mention the feedback loop for task evaluation.

\chapter{Deliverables}
% Entregables del proyecto
% Project deliverables

\section{Spring Application}
% Aplicación Spring
% Spring application
% [ ] Describe the CLI mode for running benchmarks.
% [ ] Explain the modular architecture of the backend.
% [ ] Mention the integration of LLMs as judges.

\section{React Frontend}
% Interfaz React
% React frontend
% [ ] Highlight the interactive features of the UI.
% [ ] Explain how users can add/edit tasks and explore results.

\section{Modular Benchmark}
% Benchmark modular
% Modular benchmark
% [ ] Describe the dataset format and its flexibility.
% [ ] Explain how tasks can be added or modified.
% [ ] Mention the support for multiple programming languages.

\chapter{Introduction}
% [ ] Start with a broad introduction to LLMs and their impact on software development
% [ ] Explain what benchmarks are and why they are important
% [ ] Define the problem: benchmarks are costly and inefficient to run
% [ ] Highlight the need for customizable and efficient benchmarks
% [ ] State the objectives of this work
% [ ] Outline the structure of the document

\chapter{State of the Art}
\section{Evolution of LLM Benchmarks}
% [ ] Cite "Evaluating Large Language Models Trained on Code" (HumanEval paper)
% [ ] Describe how HumanEval became the standard benchmark
% [ ] Mention its limitations and why new benchmarks were needed
% [ ] Use "Benchmarks and Metrics for Evaluations of Code Generation: A Critical Review" to classify different types of benchmarks

\section{Types of Benchmarks}
\subsection{Academic and Competitive Benchmarks}
% [ ] Describe HumanEval and its influence
% [ ] Cite "HumanEval Pro and MBPP Pro" to show evolution of benchmarks
% [ ] Explain how MBPP was improved to prevent data leakage
% [ ] Cite "One-to-many testing for code generation" to discuss test design

\subsection{Real-world Task Benchmarks}
% [ ] Cite "SWE-bench: Can Language Models Resolve Real-World GitHub Issues?"
% [ ] Explain why real-world tasks are important
% [ ] Discuss limitations of using real-world tasks

\subsection{Interactive Benchmarks}
% [ ] Cite "InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback"
% [ ] Explain the importance of feedback loops
% [ ] Describe Docker environments for interactive testing
% [ ] Cite "Copilot Arena: A Platform for Code LLM Evaluation in the Wild"
% [ ] Discuss real user interaction metrics

\section{User Experience in AI-Assisted Programming}
% [ ] Cite "Need Help? Designing Proactive AI Assistants for Programming"
% [ ] Discuss importance of timing in AI suggestions
% [ ] Cite "Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming"
% [ ] Describe 12 states of working with Copilot
% [ ] Cite "Practices and Challenges of Using GitHub Copilot: An Empirical Study"
% [ ] List main problems users face with AI assistants

\section{Problems with Existing Benchmarks}
% [ ] Cite "Do Large Language Model Benchmarks Test Reliability?"
% [ ] Discuss concept of platinum benchmarks
% [ ] List common problems:
%   - Benchmark saturation
%   - Ambiguous tasks
%   - Incorrect answers marked as correct
%   - Data leakage
% [ ] Explain why customizable benchmarks are needed

\chapter{Problem Description}
\section{Current Limitations}
% [ ] High cost of running benchmarks
% [ ] Time and energy consumption
% [ ] Inefficiency due to irrelevant tasks
% [ ] Lack of customization options
% [ ] Limited feedback and result analysis

\section{Requirements for a Solution}
% [ ] Must be modular and customizable
% [ ] Should support multiple programming languages
% [ ] Must allow task filtering and configuration
% [ ] Should provide detailed results analysis
% [ ] Must be eco-friendly and cost-efficient

\chapter{Proposed Solution}
% ...existing code for Overview section...

\section{Architecture}
% [ ] Describe the overall system architecture
% [ ] Explain the role of each component:
%   - Spring Boot backend
%   - React frontend
%   - Docker environments
%   - LLM integration
% [ ] Include a diagram showing component interaction

\section{Task Dataset Format}
% [ ] Explain the YAML/JSON structure
% [ ] Describe available task types
% [ ] List supported programming languages
% [ ] Show how to add new tasks
% [ ] Include example configurations

\section{Benchmark Configuration}
% [ ] Describe available configuration options:
%   - Task filters
%   - Programming language selection
%   - Metrics selection
%   - LLM judge configuration
% [ ] Explain how to optimize for different use cases

\chapter{Results and Evaluation}
\section{Performance Metrics}
% [ ] Describe the metrics used
% [ ] Compare with existing benchmarks
% [ ] Analyze efficiency improvements

\section{Case Studies}
% [ ] Present example benchmark runs
% [ ] Show different configuration scenarios
% [ ] Analyze results in detail

\chapter{Conclusions}
% [ ] Summarize the main contributions
% [ ] Compare with existing solutions
% [ ] Discuss limitations
% [ ] Suggest future work:
%   - Additional programming languages
%   - More task types
%   - Enhanced UI features
%   - Integration with CI/CD systems
