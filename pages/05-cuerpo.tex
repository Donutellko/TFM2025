% Checklist and TO DOs
% Describe metodology of the work with sources
% why do we need benchmarks
    % Frontier language models are not reliable on simple tasks \cite{vendrow2025largelanguagemodelbenchmarks}
% Classification of Benchmarks
% add Pros and Cons of my approach compared with anothers
% why do most benchmark use fixed sets of tasks
% why do we need a modular benchmark
% What do benchmarks check
  % Categories of tasks: Math, Logic, Table understanding,  Reading comprehension, Commonsense reasoning, Vision \cite{vendrow2025largelanguagemodelbenchmarks}
%
% Why did I chose Java and Spring Boot instead of Python (because Spring AI is great), and React
% Mention that I used OpenRouter, but any provided supported by Spring AI could be used
%
% Focus more on the concept of modular and customizable benchmarks, than in the implementation
% Describe bigcode-evaluation-harness and lm-evaluation-harness, and what lacks in them

\chapter{Introduction}

% What is missing or should be improved:
% Methodology Overview: There is no brief mention of the approach or methods used in the work. Add a short paragraph summarizing the methodology (e.g., system design, implementation, evaluation).
% Нужно описать, что я нашёл и использовал уже существующие Critical Review по теме в качетсве базы для анализа существующей литературы, затем следовал ссылкам на статьи, которые мне нужны. Последний подходящий Critical Review был опубликован в середине 2024 года, так что затем я сделал поиск по статьям за 2025 года и вторую половину 2024 года по ключевым словам, связанным с Benchmarking и LLM, выбрав среди них актуальные, отвечая на вопросы исследования. Затем, на основании собранной информации, я добавил свои собственные идеи и предложения по улучшению бенчмарков.

% ВАЖНО !!! Нужно добавить список вопросов, на которые я отвечал в ходе исследования литературы.!!

% Contributions: While objectives are listed, a concise summary of the main contributions (what is new or improved) should be added at the end of the introduction.

Large Language Models (LLMs) have revolutionized software development through AI-assisted programming tools like GitHub Copilot. However, evaluating and fine-tuning these models requires extensive benchmarking, which comes at significant computational, financial, and environmental costs. Current benchmarks often contain irrelevant tasks and provide limited customization options, making them inefficient for specific use cases.

This work explores existing benchmarks for code-generating LLMs and proposes a novel approach: modular and customizable benchmarks that can be tailored to specific needs while remaining cost-effective and environmentally conscious.

% Methodology Overview
This thesis began with a comprehensive literature review, using recent critical reviews as a foundation for analyzing the state of LLM benchmarking. The review was extended by following citations to relevant articles published in late 2024 and 2025, focusing on keywords related to benchmarking and LLMs. The findings from this literature analysis informed the design and implementation of a modular benchmarking system, which was then evaluated for efficiency, flexibility, and environmental impact.

% Research Questions
The literature review and subsequent research were guided by the following questions:
\begin{itemize}
    \item What are the main limitations of current LLM benchmarks for code generation?
    % \item How does benchmark saturation and task leaking affect the reliability of evaluation?
    \item What metrics best reflect real-world usability and code quality?
    \item How can benchmarks be made customizable for different user needs?
    \item What is the environmental impact of repeated benchmarking, and how can it be reduced?
\end{itemize}

% Contributions Summary
\textbf{Main contributions of this work:}
\begin{itemize}
    \item A critical analysis of existing LLM code generation benchmarks and their shortcomings.
    \item The design and implementation of a modular, customizable benchmarking framework.
    \item Integration of environmental and cost considerations into the benchmarking process.
    \item An interactive web interface for configuring benchmarks and analyzing results.
    \item Recommendations for future benchmarking practices based on empirical findings.
\end{itemize}

\section{Problem Statement}

Running benchmarks is essential for LLM developers, researchers and users. Developers and researchers need benchmarks to fine-tune their models or compare approaches, while users rely on them to select appropriate models and adjust parameters like temperature and system prompts. However, this process faces several challenges:
% Fine-tune key parameters like temperature, top_p, top_k to control model creativity vs determinism.
Cost and Resource Consumption: Each benchmark run consumes significant computational resources, time, and energy. As noted in recent studies, the environmental impact of repeated benchmark runs is becoming a growing concern.
% TO-DO: add a reference to the study on environmental impact

Lack of Customization: Most benchmarks are developed as fixed sets of tasks, making them difficult to adapt for specific use cases. This limitation was highlighted in the SWE-bench study~\cite{jimenez2024swebenchlanguagemodelsresolve}, which emphasized the need for more flexible evaluation methods.
% Развить мысль, как SWE сам решает эту проблему и каким образом

Inefficient Task Selection: Many benchmarks include tasks that may be irrelevant for specific applications, leading to wasted resources. Recent research \cite{vendrow2025largelanguagemodelbenchmarks} has shown that benchmark saturation often results in running unnecessary tests that all modern LLMs pass easily.

\section{Objectives}

The main objectives of this work are:
\begin{enumerate}
    \item Analyze existing benchmarks and their limitations
    \item Design a modular benchmark system that allows:
    \begin{itemize}
        % Badly written, TO-DO
        \item Custom task selection and filtering
        \item Configuration of testing criteria
        \item Support for multiple programming languages and task types
        \item Integration with CI/CD pipelines
    \end{itemize}
    \item Implement an interactive web interface for benchmark configuration and result analysis
    \item Develop a cost-efficient and environmentally conscious approach to benchmarking
\end{enumerate}

\chapter{State of the Art}

\section{Evolution of Code Generation Benchmarks}

The evolution of benchmarks for code generation has been driven by the need to evaluate the capabilities of Large Language Models (LLMs) in programming tasks.
Early benchmarks focused on isolated tasks, but as LLMs became more sophisticated, the need for more comprehensive and realistic evaluation methods emerged.

The first benchmarks were aimed at text comprehension and contained questions and expected answers, such as GLUE, SQuAD and GSM8K with grade-school math problems~(\cite{vendrow2025largelanguagemodelbenchmarks}).

As LLM capabilities expanded, benchmarks shifted towards programming tasks, with a focus on code generation and understanding.
Some pioneers in LLM code benchmarking that are MBPP, HumanEval and APPS.
\begin{itemize}
\item MBPP (Mostly Basic Python Problems) published by~\cite{austin2021program} contains $974$ crowdsourced Python programming problems with tests.

\item HumanEval was developed in OpenAI by\cite{chen2021evaluatinglargelanguagemodels} with $164$ hand-crafted problems, with a goal of avoiding data leakage (to ensure that the problems and golden solutions are not present in the training dataset).

\item APPS by ~\cite{hendrycksapps2021} features $10000$ Python tasks with $131777$ test cases, borrowed from open-access sites like Codewars and Codeforces.
\end{itemize}
These benchmarks are still used to this day for comparing performance of different models in scientific papers.
Notably, APPS is a benchmark commonly used for fine-tuning LLMs for programming tasks, as it allows to use separate sets of problems for training and evaluation~(\cite{bigcode-evaluation-harness}).

The mentioned benchmarks became less effective, as newer models were trained on the same tasks they are being evaluated on.
This phenomenon is known as \textbf{data leakage} as described by\cite{vendrow2025largelanguagemodelbenchmarks}.

Amazon's Recode benchmark~\cite{recode_wang2022} addressed this issue by introducing perturbations on docstrings, function names, and codes, while staying semantically close to the original task.
However, that is more of a way to test the robustness of the model, rather than a way to test its ability to solve brand-new problems.

More recent developments like HumanEval Pro and MBPP Pro~\cite{yu2024humanevalprombpppro} introduced more sophisticated testing approaches.
Their multistep evaluation process tests an LLM's ability to work with its own generated code.
First, an LLM generates a solution to a known problem from HumanEval or MBPP datasets.
Then, it is given a new task that requires calling a function generated in the first step.
This approach revealed limitations in some models that perform well on simpler tasks.

The mentioned academic benchmarks focus on controlled and isolated tasks, SWE-bench (\cite{jimenez2024swebenchlanguagemodelsresolve}) moved toward real-world scenarios.
Software engineering tasks were taken from resolved Issues from GitHub repositories of open-source projects Python.
SWE-bench is famous for its leaderboards, where laboratories and companies all over the world compete to achieve the higher percentage of solved tasks.
But the benchmark is limited to tasks from only 12 open-source repositories, and only supports Python programming language.

Researchers~\cite{chi2025copilotarenaplatformcode} have found a different way to evaluate LLMs in real-world scenarios.
Instead of using a fixed set of tasks, they developed a plugin called CopilotArena for an IDE.
The plugin provides two code completion options from different LLMs, and allows a user to choose the one they prefer.
This approach allows for a more realistic evaluation of LLMs in coding tasks, but it lacks the controlled environment and a solid numerical result for each model.
Such an approach could be useful for A/B testing of LLMs in production, but it is not suitable for scientific research and repeated evaluations during fine-tuning.

The benchmarks mentioned above perform in a static environment, where the model is given a task and expected to generate a solution.
The InterCode framework by\cite{yang2023intercodestandardizingbenchmarkinginteractive} introduces interactive environments using Docker.
This enables evaluation of LLMs in realistic and interactive development scenarios with compilation and runtime feedback.
The environments and scenarios were prepared for Python, SQL, and BASH, but the framework allows introducing new environments and scenarios.
This approach more closely mirrors actual developer workflows and allows for testing LLMs in the role of a partially independent agent.
But this approach comes with increased computational overhead of running a Docker environment, a virtual operating system, and an instance of a database, which limits the overall speed and a number of scenarios that can be tested at once.

Many of the mentioned benchmarks have inspired researchers to implement new benchmarks based on them.
That could be their adaptations in other programming languages or refined datasets with verified and new hand-crafted tasks.
Such as in the case of SWE-bench and following \textit{SWE-bench Verified} and \textit{Multi-swe-bench}.


\section{Classification of LLM Benchmarks}

% TO-DO add classifications

%Another article~\cite{mozannar2024readinglinesmodelinguser} finds out that a user can often accept a first proposed solution in order to see it with proper syntax highlighting and be able to understand it better. But later he could even remove the suggestion and waits for a repeated completion or simply writes the piece of the code themself. Although that should not be considered a significant flaw of the benchmark, it could partially skew the results.

Based on the analysis of existing benchmarks, we can identify several key limitations that our work aims to address:
\begin{itemize}
    \item Benchmark saturation,
%: Many tasks become irrelevant as models improve\cite{vendrow2025largelanguagemodelbenchmarks};
    \item Data leakage,
%: LLMs are trained on benchmark tasks solutions, making them obsolete\cite{vendrow2025largelanguagemodelbenchmarks};
    \item Limited feedback,
%: Most benchmarks provide only pass/fail results;
    \item High resources consumption,
%: Running comprehensive benchmarks is expensive and time-consuming;
    \item Environmental impact,
%: Repeated runs contribute to unnecessary carbon emissions;
%    \item Inflexibility,
%%: Fixed task sets don't adapt to specific needs;
    \item Error-proneness tasks,
%: benchmarks contain up to 5 percent of mislabelled or erroneous tasks\cite{vendrow2025largelanguagemodelbenchmarks}.
    \item Limited feedback and output.
\end{itemize}

Let's address each of these limitations one by one.

\subsection{Benchmark Saturation}

When a benchmark becomes saturated, it means that the tasks in the benchmark are too easy for the current state-of-the-art LLMs, leading to high pass rates and diminishing returns on further improvements.
It can be caused by either advances in LLMs or by data leakage, where the tasks and their solutions are present in the training datasets of the models being evaluated.

At some moment, testing on the simplest tasks becomes irrelevant, as all models pass them with high scores.
Some datasets contain \textbf{metadata} that allows filtering out tasks based on their difficulty, thus saving resources and time on each evaluation.

\subsection{Data Leakage}

Benchmark saturation mentioned in the above sections is partially explained by advances in models, but it can also be attributed to information \textbf{leaking}: the popular and publicly available benchmarks appear in the training datasets accompanied by the golden solutions.
This leads to a situation where the models are trained on the same tasks they are being evaluated on.

There are several ways to avoid the consequences of data leakage:
\begin{itemize}
    \item hand-crafting brand-new tasks without publishing them or using for in-house training;
    \item generating new tasks based on the existing ones as it was done with HumanEval Pro and MBPP Pro;
    \item or perturbing existing tasks as it was done in ReCode.
\end{itemize}

\subsection{High Cost and Environmental Impact}

Repeated training and benchmarking of LLMs require significant computational resources, leading to significant electricity consumption and carbon emissions.
This environmental impact is increasingly important in the context of global efforts to reduce carbon footprints.
We will want for benchmarks to account for these factors and encourage more sustainable evaluation practices.
% TO-DO: Добавить какое-то исследование на эту тему.

There are leaderboards that account for $CO_2$ emissions, such as Hugging Face\cite{huggingfaceCalculation}, which tracks the carbon footprint of using models.
However, these metrics are often not integrated into traditional benchmarks, leading to a lack of awareness about the environmental impact of LLM evaluation practices.

The most common metric in benchmarks is pass@k that measures the percentage of correct solutions among the $k$ solutions generated by the model.
This implies that for each task in the benchmark dataset, a model repeatedly generates a number of solutions, just to receive a single numeric result to use for a metric.
This metric is used in ClassEval, MBPP, MathQA-Python, CoderEval, and HumanEval+.
Notably, HumanEval and HumanEval+ use $k=100$ (\texttt{pass@100}).
However, as Miah and Zhu~\cite{miah2024usercentricevaluationcode} pointed out, users do not normally run the LLM several times, so pass@k does not reflect its usability.

\subsection{Limited Feedback and Output}

This limitation is intertwined with the high cost.
The output of most benchmarks is a single numeric metric, such as pass@k, which indicates the percentage of tasks solved correctly by the model.
Compared to the amount of work and energy that was consumed to produce this result, and the amount of information that could be extracted from the model's responses and test runs, this approach is very limited.

For example, SWE leaderboard~\cite{swebenchSWEbenchLeaderboards} is created based on a single number that does not reflect the types of tasks that the model is better or worse at solving~\cite{miah2024usercentricevaluationcode}.
Thus, a researcher or a user might choose a suboptimal model for their specific needs, resulting in lower performance or higher cost.
This is partially countered by websites that aggregate results on several benchmarks~\cite{vellumLeaderboard2025} which can give a very high-level picture.

Some of the ways to gather more information from the model's responses are:
\begin{itemize}
    \item Gather performance metrics for each task, such as execution time, memory usage, and CPU load;
    \item Count the number of input and output tokens used for each generation to compare cost-effectiveness of models;
    \item Analyze the generated code for style and quality, such as cyclomatic complexity, number of lines, and code duplication;
    \item Provide a way to analyze individual task failures, such as incorrect solutions, timeouts, and exceptions;
    \item Using LLM-as-a-judge approach to evaluate the quality of the generated solution.
\end{itemize}

\subsection{Error-Proneness of Tasks}

When creating and managing big datasets, errors are inevitable.
As\cite{vendrow2025largelanguagemodelbenchmarks} found out, popular benchmarks contain up to 5 percent of mislabeled or erroneous tasks.
This can lead to incorrect evaluation results and misinterpretation of model capabilities.

To mitigate this issue, a researcher should be able to examine the failures and more easily spot the errors in the tasks.
This will also allow the researcher to spot patterns in model's errors, and possibly mitigate them by improving training datasets, updating a system prompt, and adjusting temperature and other parameters.

\section{Problems in LLM Benchmarking}

% Задачки в HumanEval уже общеизвестны, так что ЛЛМ натренированы на их решениях. Зато HumanEval Pro добавили дополнительный шаг в задачу, что продемонстрировало, что даже самые современные LLMs могут не справляться с незнакомыми задачами, или переиспользовать написанный ими же код, вызывая его в качестве функции. Таким образом, разработка новых задач важна, и нужно предоставить пользователю бенчмарка удобный способ добавлять и модифицировать задачи в бенчмарке.

Benchmark saturation mentioned in the above sections is partially explained by advances in models, but it can also be attributed to information \textbf{leaking}: the popular and publicly available benchmarks appear in the training datasets accompanied by the golden solutions.
Even then, it doesn't mean that an LLM won't struggle when presented with the same task.
When changing the task phrasing while keeping the semantic consistent, there is a 4.5-percent drop in solvability, showing that the models remember the phrasing of the descriptions in the original dataset. \cite{uniyal2024one}

One of the important aspects of LLM evaluation is the choice of the metrics.
For code quality, there are BLEU, CodeBLEU, RUBY, ROUGE-L, METEOR, ChrF\@.
They assess the similarity of the generated code to the golden solution, taking into account the properties of source code.
Evtikhiev et al.~\cite{evtikhiev2023out} takes 6 metrics, commonly used in papers.
However, the authours conduct a study, comparing the results of metrics with human evaluation of the solutions.
The results suggest that none of the analyzed metrics can correctly emulate human judgment, but ChrF metric is considered better than the others commonly used in papers.

A paper by\cite{crupi2025effectiveness}looks into an approach of using LLM to evaluate the quality of the solution generated by another model (LLM-as-a-judge approach).
As a result, they come to a conclusion that LLM-as-a-judge is a substantial improvement over mentioned metrics, and GPT-4-turbo can mimic closely a human evaluation.

% remove or rephrase: Recent studies have revealed the complexity of user interaction with AI programming assistants. Research by \cite{chen2025needhelpdesigningproactive} identified specific moments when developers expect proactive suggestions from LLMs, while \cite{mozannar2024readinglinesmodelinguser} classified 12 distinct states of interaction with tools like GitHub Copilot.
% И что с того, что идентифицировал? Тогда нужно развить мысль, но вообще я не решаю эту проблему в своей работе

\section{Existing Benchmarking Frameworks and Their Limitations}

Apart from the benchmarks themselves, there are several frameworks that facilitate LLM evaluation.
These frameworks provide tools for running benchmarks and collecting results.

Two widely used benchmarking frameworks are \textbf{bigcode-evaluation-harness}~\cite{bigcode-evaluation-harness} and \textbf{lm-evaluation-harness}~\cite{githubGitHubEleutherAIlmevaluationharness}.
Both provide tools for running standardized benchmarks on LLMs, but they have notable limitations.
The lm-evaluation-harness is more general-purpose and supports a broader range of language tasks, yet it also relies on fixed task sets and lacks modularity for user-defined benchmarks.
Neither framework provides built-in support for environmental metrics or fine-grained task selection, highlighting the need for more flexible and sustainable benchmarking solutions.

In the Table\ref{tab:framework-comparison} we compare the two frameworks based on their features and limitations.

\begin{table}[H]
    \centering
    \begin{tabular}{|p{4cm}|p{5cm}|p{5cm}|}
        \hline
        \textbf{Feature} & \textbf{bigcode-evaluation-harness} & \textbf{lm-evaluation-harness} \\
        \hline
        Specialization & Majorly, code writing tasks, but also allows for documentation generation tasks and natural language reasoning tasks & A universal harness supporting a wide range of tasks \\
        \hline
        Included benchmarks & MBPP, MBPP+, DS-1000, MultiPL-E, Mercury, GSM8K, etc. & MBPP, MBPP+, HumanEval, SpanishBench, basqueGLUE, and many more. \\
        \hline
        Defining new tasks & Requires source code modification & Requires source code modification \\
        \hline
        Available configuration & Task dataset name, Number of tasks, Temperature, Saving LLM responses, Limit of LLM response & Task datasets list,  \\
        \hline
        Run interface & CLI-based, no GUI & CLI-based, no GUI \\
        \hline
        Result analysis & Overall numeric metric and LLM responses saved as a file &  \\
        \hline
        Visualization & No visualization tools & No visualization tools \\
        \hline
        Evaluation of multiple LLMs & One model per run & One model per run \\
        \hline
        Supports model loading via transformers & Yes & Yes \\
        \hline
    \end{tabular}
    \caption{Comparison of bigcode-evaluation-harness and lm-evaluation-harness}
    \label{tab:framework-comparison}
\end{table}


\chapter{Problem Description}

In the previous chapter, we looked at the existing popular benchmarking frameworks.
Based on the list of their features, we can create a list of limitations that we can address in our work:
\begin{itemize}
    \item A more broad and detailed output of the results, instead of simple numeric outputs.
    \item A flexible and interactive web interface for configuring benchmarks, selecting tasks, and visualizing results, or diving deeper into the failures.
    \item An easier approach for defining new tasks or modifying the existing ones, without the need to modify the source code of the framework.
    \item TO-DO
\end{itemize}

\section{Analysis of Current Limitations}

The limitations of existing benchmarks extend beyond just inefficiency. Studies like  have identified several critical issues:

% TO DO: check if all that is mentioned in the paper


% Describe how do I address these limitations in my work

\section{Requirements for a Solution}

Based on the analyzed limitations and user needs, we identified key requirements for a more effective benchmarking approach:

Modularity:
\begin{itemize}
    \item Support for multiple programming languages
    \item Ability to add/modify tasks easily
    \item Configurable testing criteria
\end{itemize}

Efficiency:
\begin{itemize}
    \item Task filtering capabilities
    \item Optimized resource usage
    \item Quick feedback loops
\end{itemize}

User Experience:
\begin{itemize}
    \item Interactive configuration interface
    \item Detailed result analysis
    \item Integration with development workflows
\end{itemize}

The benchmark should:
\begin{itemize}
    \item Provide a way to analyse individual task failures
    \item Allow users to select tasks based on their specific needs
    \item Get the most information from each generated solution
    \item ...
\end{itemize}

\chapter{Proposed Solution}

\section{Architecture Overview}

Our solution combines three main components:
\begin{itemize}
    \item A Spring Boot backend with MVC architecture
    \item A React-based frontend for configuration and visualization
    \item Docker environments for isolated task execution
\end{itemize}

\begin{figure}[h]
    \centering
    % Add architecture diagram placeholder
    \caption{System Architecture Overview}
    \label{fig:architecture}
\end{figure}

\section{Task Dataset Format}

Tasks are defined in YAML format, allowing for easy modification and extension:

% Example structure (full examples in Appendix)
% TODO put into Appendix
\begin{verbatim}
tasks:
  - name: "Example Task"
    type: "implementation"
    difficulty: "medium"
    languages: ["python", "java"]
    parameters:
      use_libraries: false
      generate_tests: true
\end{verbatim}

\section{Interactive Configuration}

The React frontend provides:
- Task filtering by type, difficulty, and language
- Testing criteria selection
- Resource usage configuration
- Result visualization

\chapter{Implementation Details}

\section{Project Structure}

The project follows a modern microservices architecture with separate backend and frontend applications.
Currently, the configuration, status and result files are stored in a file-based storage system, but they can be easily adapted to use a database if needed.

\begin{verbatim}
Project Root
|-- src/             # Spring Boot application source code
|-- frontend/        # React application source code
|-- exec_configs/    # Execution configuration YAML files
|-- task_sources/    # Task source YAML files
|-- bench_status/    # Benchmark status files
`-- bench_results/   # Benchmark results
\end{verbatim}

\section{Backend Development}

The Spring Boot application serves as the core of the system, providing:

\subsection{REST API Endpoints}
The backend exposes a comprehensive RESTful API:
\begin{itemize}
    \item \texttt{/api/configs}: Configuration file management
    \item \texttt{/api/tasks}: Task source file operations
    \item \texttt{/api/benchmarks}: Benchmark execution control
    \item \texttt{/api/status}: Status monitoring for current benchmarks run
    \item \texttt{/api/results}: Result retrieval for visualization and analysis
\end{itemize}

\subsection{Core Functionality}
\begin{itemize}
    \item Task processing and execution in isolated environments
    \item Docker container management for language-specific runtimes
    \item File-based storage system for configurations and results
    \item Integration with LLM judges through Spring AI
\end{itemize}

\begin{figure}[h]
    \centering
    % Add backend architecture diagram
    \caption{Backend Component Architecture}
    \label{fig:backend-arch}
\end{figure}

\section{Frontend Development}

The React frontend, built with TypeScript, provides an intuitive interface for:

\subsection{User Interface Components}
\begin{itemize}
    \item Configuration file upload and management
    \item Task source file handling
    \item Real-time benchmark monitoring
    \item Result visualization and analysis
\end{itemize}

\subsection{Interactive Features}
The application enables users to:
\begin{itemize}
    \item Upload and manage YAML configurations
    \item Configure benchmark parameters interactively
    \item Monitor benchmark progress in real-time
    \item Analyze and export results
\end{itemize}

\begin{figure}[h]
    \centering
    % Add frontend screenshot
    \caption{Frontend Interface Overview}
    \label{fig:frontend-ui}
\end{figure}

\section{Deployment}

Both components support flexible deployment options:

\subsection{Local Development}
\begin{verbatim}
# Backend
mvn spring-boot:run

# Frontend
cd frontend
npm install
npm run dev
\end{verbatim}

\subsection{Docker Deployment}
The system includes Docker configurations for containerized deployment:

\begin{verbatim}
# Backend container
docker build -t ai-benchmark-backend -f Dockerfile.backend .
docker run -p 8080:8080 ai-benchmark-backend

# Frontend container
docker build -t ai-benchmark-frontend -f Dockerfile.frontend .
docker run -p 5173:5173 ai-benchmark-frontend
\end{verbatim}

\chapter{Results and Evaluation}

\section{Performance Analysis}

We evaluated our solution against traditional benchmarks:

\begin{table}[h]
    \centering
    % Add comparison table placeholder
    \caption{Comparison with Traditional Benchmarks}
    \label{tab:comparison}
\end{table}

Key metrics:
\begin{itemize}
    \item Execution time per task
    \item Resource usage (CPU, memory)
    \item Cost per benchmark run
    \item Environmental impact (estimated $CO_2$ emissions)
\end{itemize}

\section{Case Studies}

We present three scenarios demonstrating the system's flexibility:
\begin{enumerate}
    \item Fine-tuning an LLM for specific programming tasks
    \item Evaluating code style consistency
    \item Testing real-world project integration
\end{enumerate}

\chapter{Conclusions}

\section{Contributions}

Our modular benchmark approach offers several advantages:
\begin{itemize}
    \item Customizable evaluation scenarios
    \item Reduced resource consumption
    \item Improved feedback quality
    \item Easy integration with development workflows
\end{itemize}

\section{Future Work}

Potential improvements include:
\begin{itemize}
    \item Additional programming language support
    \item Enhanced visualization tools
    \item CI/CD pipeline integration
    \item Advanced metrics collection
\end{itemize}

\section{Impact}

This work contributes to more sustainable and efficient LLM evaluation practices, potentially reducing both costs and environmental impact while providing more meaningful results.
